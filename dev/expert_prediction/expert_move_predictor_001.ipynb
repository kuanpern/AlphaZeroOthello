{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner 001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* original tournament data are obtained from http://www.ffothello.org/informatique/la-base-wthor/\n",
    "* encoded dataset used in this notebook can be downloaded at https://drive.google.com/open?id=1thIFevwYhD9Y9JIMvLPS6QF0TiIzNbKR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy\n",
    "import glob\n",
    "import random\n",
    "import copy\n",
    "import dill\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define action - label relationship here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2move = {0: [0, 0], 1: [0, 1], 2: [0, 2], 3: [0, 3], 4: [0, 4], 5: [0, 5], 6: [0, 6], \n",
    "              7: [0, 7], 8: [1, 0], 9: [1, 1], 10: [1, 2], 11: [1, 3], 12: [1, 4], 13: [1, 5], \n",
    "              14: [1, 6], 15: [1, 7], 16: [2, 0], 17: [2, 1], 18: [2, 2], 19: [2, 3], 20: [2, 4], \n",
    "              21: [2, 5], 22: [2, 6], 23: [2, 7], 24: [3, 0], 25: [3, 1], 26: [3, 2], 27: [3, 5], \n",
    "              28: [3, 6], 29: [3, 7], 30: [4, 0], 31: [4, 1], 32: [4, 2], 33: [4, 5], 34: [4, 6], \n",
    "              35: [4, 7], 36: [5, 0], 37: [5, 1], 38: [5, 2], 39: [5, 3], 40: [5, 4], 41: [5, 5], \n",
    "              42: [5, 6], 43: [5, 7], 44: [6, 0], 45: [6, 1], 46: [6, 2], 47: [6, 3], 48: [6, 4], \n",
    "              49: [6, 5], 50: [6, 6], 51: [6, 7], 52: [7, 0], 53: [7, 1], 54: [7, 2], 55: [7, 3], \n",
    "              56: [7, 4], 57: [7, 5], 58: [7, 6], 59: [7, 7], 60: 'PASS'}\n",
    "move2label = {tuple(q): p for p, q in label2move.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NETWORK ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define functionals\n",
    "        self.fc1     = torch.nn.Linear(64, 1000)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.fc2     = torch.nn.Linear(1000, 62)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    # end def\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    # end def\n",
    "# end class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZERO_Loss(torch.nn.Module):        \n",
    "    def forward(self, outputs, labels):\n",
    "        # get the batch size\n",
    "        batch_n = outputs.shape[0]\n",
    "\n",
    "        move_probs, pred_vals = torch.split(outputs, (61, 1), 1)\n",
    "        search_probs, winners = torch.split(labels , (61, 1), 1)\n",
    "\n",
    "        # compute the loss function\n",
    "        pi  = search_probs.contiguous().view(-1).float()\n",
    "        logp = torch.log(move_probs).contiguous().view(-1).float()\n",
    "\n",
    "        loss = torch.pow(pred_vals - winners, 2).sum() - pi.dot(logp)\n",
    "        loss = loss / batch_n\n",
    "\n",
    "        return loss\n",
    "    # end def\n",
    "# end class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "LEARNING_RATE = 0.02\n",
    "MOMENTUM      = 0.9\n",
    "BATCH_SIZE    = 200\n",
    "EPOCH_N       = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "######### Define network and optimizer ###########\n",
    "##################################################\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "criterion = AlphaZERO_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload net to device\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome2array(Y):\n",
    "    output = []\n",
    "    for i in range(len(Y)):\n",
    "        _move, _winner = Y[i]\n",
    "        _move = tuple(_move)\n",
    "        if _winner == -1:\n",
    "            _winner = 0\n",
    "        # end if\n",
    "        \n",
    "        out = numpy.zeros(62)\n",
    "        out[move2label[_move]] = 1\n",
    "        out[-1] = _winner\n",
    "        \n",
    "        output.append(out)\n",
    "    # end for\n",
    "    return numpy.array(output)\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file(filename):\n",
    "    with open(filename) as fin:\n",
    "        content = fin.read().splitlines()\n",
    "    # end with\n",
    "\n",
    "    X, Y = [[], []]\n",
    "    for row in content:\n",
    "        x, y = json.loads(row)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    # end for\n",
    "\n",
    "    # convert to numpy array\n",
    "    X = numpy.array(X)\n",
    "    Y = outcome2array(Y)\n",
    "\n",
    "    return X, Y\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_some_data(files, N=20):\n",
    "    input_data = []\n",
    "    \n",
    "    ## GET INPUT DATA ##\n",
    "    # select input files from directory randomly\n",
    "    sel_files = [random.choice(files) for _ in range(N)]\n",
    "    data = [read_data_from_file(_file) for _file in sel_files]\n",
    "    X, Y = [[], []]\n",
    "    for x, y in data:\n",
    "        X.extend(x)\n",
    "        Y.extend(y)\n",
    "    # end for\n",
    "    return X, Y\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = 'data/labelled'\n",
    "files = glob.glob(indir+'/*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    output = []\n",
    "    for i in range(0, len(l), n):\n",
    "        output.append(l[i:i+n])\n",
    "    # end for \n",
    "    return output\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(files, BATCH_SIZE, n=5):\n",
    "    stime = time.time()\n",
    "    #logger.info(' - selecting data sets for training')\n",
    "\n",
    "    # read data\n",
    "    X, Y = read_some_data(files=files)\n",
    "    Xs = chunks(X, BATCH_SIZE)\n",
    "    Ys = chunks(Y, BATCH_SIZE)\n",
    "\n",
    "    #logger.info(' - start training')\n",
    "    for _iter in range(n):\n",
    "        running_loss = 0\n",
    "        for i in range(len(Xs)):\n",
    "            inputs = torch.from_numpy(numpy.array(Xs[i])).float()\n",
    "            labels = torch.from_numpy(numpy.array(Ys[i])).float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 0.25)\n",
    "            running_loss += loss.item()\n",
    "        # end for\n",
    "        loss = running_loss / len(Xs)\n",
    "        #logger.info('[iteration %d] loss: %.3f' % (_iter, loss))\n",
    "    # end for\n",
    "    return loss\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(note: please ensure the database file and table are created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = './data/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(20000):\n",
    "    loss = epoch(files, BATCH_SIZE)\n",
    "    ###################\n",
    "    # LOSS MONITORING #\n",
    "    ###################\n",
    "    if i % 200 == 0:\n",
    "        losses.append(loss)\n",
    "        print('iteration: %d | loss: %4.3f | time: %4.1f' % (i, loss, time.time()-stime))\n",
    "    # end if\n",
    "\n",
    "    ####################\n",
    "    # MODEL VERSIONING #\n",
    "    ####################\n",
    "    if i % 500 == 0:\n",
    "        name='oth_exp_pred-iter'+str(i).zfill(5)\n",
    "        # save model\n",
    "        outfile = outdir+'/'+name+'.dill'\n",
    "        with open(outfile, 'wb') as fout:\n",
    "            dill.dump(copy.deepcopy(net), fout)\n",
    "        # end with\n",
    "    # end if\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
